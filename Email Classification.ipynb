{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93cdbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Unique Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8299b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03c62a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 files\n",
      "Processed 200 files\n",
      "Processed 300 files\n",
      "Processed 400 files\n",
      "Processed 500 files\n",
      "Processed 600 files\n",
      "Processed 700 files\n",
      "Processed 800 files\n",
      "Processed 900 files\n",
      "Processed 1000 files\n",
      "Processed 1100 files\n",
      "Processed 1200 files\n",
      "Processed 1300 files\n",
      "Processed 1400 files\n",
      "Processed 1500 files\n",
      "Processed 1600 files\n",
      "Processed 1700 files\n",
      "Processed 1800 files\n",
      "Processed 1900 files\n",
      "Processed 2000 files\n",
      "Processed 2100 files\n",
      "Processed 2200 files\n",
      "Processed 2300 files\n",
      "Processed 2400 files\n",
      "Processed 2500 files\n",
      "Processed 2600 files\n",
      "Processed 2700 files\n",
      "Processed 2800 files\n",
      "Processed 2900 files\n",
      "Processed 3000 files\n",
      "Processed 3100 files\n",
      "Processed 3200 files\n",
      "Processed 3300 files\n",
      "Processed 3400 files\n",
      "Processed 3500 files\n",
      "Processed 3600 files\n",
      "Processed 3700 files\n",
      "Processed 3800 files\n",
      "Processed 3900 files\n",
      "Processed 4000 files\n",
      "Processed 4100 files\n",
      "Processed 4200 files\n",
      "Processed 4300 files\n",
      "Processed 4400 files\n",
      "Processed 4500 files\n",
      "Processed 4600 files\n",
      "Processed 4700 files\n",
      "Processed 4800 files\n",
      "Processed 4900 files\n",
      "Processed 5000 files\n",
      "Processed 5100 files\n",
      "Processed 5200 files\n",
      "Processed 5300 files\n",
      "Processed 5400 files\n",
      "Processed 5500 files\n",
      "Processed 5600 files\n",
      "Processed 5700 files\n",
      "Processed 5800 files\n",
      "Processed 5900 files\n",
      "Processed 6000 files\n",
      "Processed 6100 files\n",
      "Processed 6200 files\n",
      "Processed 6300 files\n",
      "Processed 6400 files\n",
      "Processed 6500 files\n",
      "Processed 6600 files\n",
      "Processed 6700 files\n",
      "Processed 6800 files\n",
      "Processed 6900 files\n",
      "Processed 7000 files\n",
      "Processed 7100 files\n",
      "Processed 7200 files\n",
      "Processed 7300 files\n",
      "Processed 7400 files\n",
      "Processed 7500 files\n",
      "Processed 7600 files\n",
      "Processed 7700 files\n",
      "Processed 7800 files\n",
      "Processed 7900 files\n",
      "Processed 8000 files\n",
      "Processed 8100 files\n",
      "Processed 8200 files\n",
      "Processed 8300 files\n",
      "Processed 8400 files\n",
      "Processed 8500 files\n",
      "Processed 8600 files\n",
      "Processed 8700 files\n",
      "Processed 8800 files\n",
      "Processed 8900 files\n",
      "Processed 9000 files\n",
      "Processed 9100 files\n",
      "Processed 9200 files\n",
      "Processed 9300 files\n",
      "Processed 9400 files\n",
      "Processed 9500 files\n",
      "Processed 9600 files\n",
      "Processed 9700 files\n",
      "Processed 9800 files\n",
      "Processed 9900 files\n",
      "Processed 10000 files\n",
      "Processed 10100 files\n",
      "Processed 10200 files\n",
      "Processed 10300 files\n",
      "Time (in seconds) to preprocess the emails: 175.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import string\n",
    "import operator\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Function to clean up text\n",
    "def text_cleanup(text):\n",
    "    text_without_punctuation = [c for c in text if c not in string.punctuation]\n",
    "    text_without_punctuation = ''.join(text_without_punctuation)\n",
    "    text_without_stopwords = [word for word in text_without_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    text_without_stopwords = ' '.join(text_without_stopwords)\n",
    "    cleaned_text = [word.lower() for word in text_without_stopwords.split()]\n",
    "    return cleaned_text\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "k = 0\n",
    "count = {}\n",
    "\n",
    "# Directory where email files are stored\n",
    "directory_in_str = r\"C:\\Users\\Lenovo\\OneDrive\\Desktop\\emails\"\n",
    "\n",
    "# Loop through files in the directory\n",
    "for file in os.listdir(directory_in_str):\n",
    "    file_name = os.path.join(directory_in_str, file)\n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding='utf-8', errors='ignore') as file_reading:\n",
    "            words = text_cleanup(file_reading.read())\n",
    "            for word in words:\n",
    "                if not (word.isdigit() or len(word) <= 2):\n",
    "                    word = lmtzr.lemmatize(word)\n",
    "                    count[word] = count.get(word, 0) + 1\n",
    "        k += 1\n",
    "        if k % 100 == 0:\n",
    "            print(\"Processed\", k, \"files\")\n",
    "    except Exception as e:\n",
    "        print(\"Error processing file:\", file_name, \"-\", e)\n",
    "\n",
    "sorted_count = sorted(count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_count = dict(sorted_count)\n",
    "\n",
    "with open(\"wordslist.csv\", \"w+\") as f:\n",
    "    f.write('word,count\\n')\n",
    "    for word, times in sorted_count.items():\n",
    "        if times < 100:\n",
    "            break\n",
    "        f.write(f\"{word},{times}\\n\")\n",
    "\n",
    "print('Time (in seconds) to preprocess the emails:', round(time.time() - start_time, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb5272c",
   "metadata": {},
   "source": [
    "#Processing to find occurence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981acd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f565aae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "Done 1500\n",
      "Done 1600\n",
      "Done 1700\n",
      "Done 1800\n",
      "Done 1900\n",
      "Done 2000\n",
      "Done 2100\n",
      "Done 2200\n",
      "Done 2300\n",
      "Done 2400\n",
      "Done 2500\n",
      "Done 2600\n",
      "Done 2700\n",
      "Done 2800\n",
      "Done 2900\n",
      "Done 3000\n",
      "Done 3100\n",
      "Done 3200\n",
      "Done 3300\n",
      "Done 3400\n",
      "Done 3500\n",
      "Done 3600\n",
      "Done 3700\n",
      "Done 3800\n",
      "Done 3900\n",
      "Done 4000\n",
      "Done 4100\n",
      "Done 4200\n",
      "Done 4300\n",
      "Done 4400\n",
      "Done 4500\n",
      "Done 4600\n",
      "Done 4700\n",
      "Done 4800\n",
      "Done 4900\n",
      "Done 5000\n",
      "Done 5100\n",
      "Done 5200\n",
      "Done 5300\n",
      "Done 5400\n",
      "Done 5500\n",
      "Done 5600\n",
      "Done 5700\n",
      "Done 5800\n",
      "Done 5900\n",
      "Done 6000\n",
      "Done 6100\n",
      "Done 6200\n",
      "Done 6300\n",
      "Done 6400\n",
      "Done 6500\n",
      "Done 6600\n",
      "Done 6700\n",
      "Done 6800\n",
      "Done 6900\n",
      "Done 7000\n",
      "Done 7100\n",
      "Done 7200\n",
      "Done 7300\n",
      "Done 7400\n",
      "Done 7500\n",
      "Done 7600\n",
      "Done 7700\n",
      "Done 7800\n",
      "Done 7900\n",
      "Done 8000\n",
      "Done 8100\n",
      "Done 8200\n",
      "Done 8300\n",
      "Done 8400\n",
      "Done 8500\n",
      "Done 8600\n",
      "Done 8700\n",
      "Done 8800\n",
      "Done 8900\n",
      "Done 9000\n",
      "Done 9100\n",
      "Done 9200\n",
      "Done 9300\n",
      "Done 9400\n",
      "Done 9500\n",
      "Done 9600\n",
      "Done 9700\n",
      "Done 9800\n",
      "Done 9900\n",
      "Done 10000\n",
      "Done 10100\n",
      "Done 10200\n",
      "Done 10300\n",
      "Time (in seconds) to segregate entire dataset to form input vector 919.42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "df = pd.read_csv('wordslist.csv', header=0)\n",
    "words = df['word']\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Specify the directory where email files are stored\n",
    "directory_in_str = r\"C:\\Users\\Lenovo\\OneDrive\\Desktop\\emails\"\n",
    "directory = os.fsencode(directory_in_str)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_in_str):\n",
    "    print(f\"Directory '{directory_in_str}' does not exist.\")\n",
    "    # Handle the error, e.g., exit the script or create the directory\n",
    "\n",
    "f = open(\"frequency.csv\", \"w+\")\n",
    "for i in words:\n",
    "    f.write(str(i) + ',')\n",
    "f.write('output')\n",
    "f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "k = 0\n",
    "for file in os.listdir(directory):\n",
    "    file_name = os.path.join(directory_in_str, file.decode(\"utf-8\"))\n",
    "    k += 1\n",
    "    file_reading = open(file_name, \"r\", encoding='utf-8', errors='ignore')\n",
    "    words_list_array = np.zeros(words.size)\n",
    "    for word in file_reading.read().split():\n",
    "        word = lmtzr.lemmatize(word.lower())\n",
    "        if(word in stopwords.words('english') or word in string.punctuation or len(word) <= 2 or word.isdigit()):\n",
    "            continue\n",
    "        for i in range(words.size):\n",
    "            if(words[i] == word):\n",
    "                words_list_array[i] = words_list_array[i] + 1\n",
    "                break\n",
    "    f = open(\"frequency.csv\", \"a\")\n",
    "    for i in range(words.size):\n",
    "        f.write(str(int(words_list_array[i])) + ',')\n",
    "    if(len(file_name) == 68):\n",
    "        f.write(\"-1\")\n",
    "    elif(len(file_name) == 71):\n",
    "        f.write(\"1\")\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "    if(k % 100 == 0):\n",
    "        print(\"Done \" + str(k))\n",
    "\n",
    "print(\"Time (in seconds) to segregate entire dataset to form input vector \" +\n",
    "      str(round(time() - start_time, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cfb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16ebfa20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cvxopt\n",
      "  Obtaining dependency information for cvxopt from https://files.pythonhosted.org/packages/a3/52/2237d72cf007e6c36367ab8a776388a9f13511e4cfa8a71b79101ad6e0fa/cvxopt-1.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cvxopt-1.3.2-cp311-cp311-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading cvxopt-1.3.2-cp311-cp311-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.8 MB 388.9 kB/s eta 0:00:33\n",
      "    --------------------------------------- 0.2/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.5/12.8 MB 2.7 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.8/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.2/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.7/12.8 MB 4.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.2/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.4/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.5/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.3/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.2/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.6/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 5.0/12.8 MB 2.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.6/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 6.0/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.4/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.9/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.4/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.8/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.4/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.0/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.6/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.9/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.2/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.6/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.7/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.4/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 7.5 MB/s eta 0:00:00\n",
      "Installing collected packages: cvxopt\n",
      "Successfully installed cvxopt-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cvxopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e9b15e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Obtaining dependency information for prettytable from https://files.pythonhosted.org/packages/3d/c4/a32f4bf44faf95accbbd5d7864ddef9e289749a8efbc3adaad4a4671779a/prettytable-3.10.0-py3-none-any.whl.metadata\n",
      "  Downloading prettytable-3.10.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from prettytable) (0.2.5)\n",
      "Downloading prettytable-3.10.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install prettytable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b25427f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 222\u001b[0m\n\u001b[0;32m    220\u001b[0m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m    221\u001b[0m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m j\n\u001b[1;32m--> 222\u001b[0m matrix , result \u001b[38;5;241m=\u001b[39m implementSVM(X_train,Y_train,X_test,Y_test,parameters,\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    223\u001b[0m write_to_file(matrix,result,parameters,\u001b[38;5;28mtype\u001b[39m,start_time)\n\u001b[0;32m    224\u001b[0m k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[20], line 149\u001b[0m, in \u001b[0;36mimplementSVM\u001b[1;34m(X_train, Y_train, X_test, Y_test, parameters, type)\u001b[0m\n\u001b[0;32m    147\u001b[0m     offset \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    148\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m SVMTrainer(Kernel\u001b[38;5;241m.\u001b[39mpolykernel(dimension,offset),\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m--> 149\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(X_train,Y_train)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    151\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m SVMTrainer(Kernel\u001b[38;5;241m.\u001b[39mlinear(),\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 40\u001b[0m, in \u001b[0;36mSVMTrainer.train\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Training function\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Caluculate the langrange multipliers\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     lagrange_multipliers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_multipliers(X, y)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Trainer returns SVM predictor that is used to predict which class the test element belongs to\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_predictor(X, y, lagrange_multipliers)\n",
      "Cell \u001b[1;32mIn[20], line 92\u001b[0m, in \u001b[0;36mSVMTrainer.compute_multipliers\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     90\u001b[0m A \u001b[38;5;241m=\u001b[39m cvxopt\u001b[38;5;241m.\u001b[39mmatrix(y, (\u001b[38;5;241m1\u001b[39m, n_samples) , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     91\u001b[0m b \u001b[38;5;241m=\u001b[39m cvxopt\u001b[38;5;241m.\u001b[39mmatrix(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m solution \u001b[38;5;241m=\u001b[39m cvxopt\u001b[38;5;241m.\u001b[39msolvers\u001b[38;5;241m.\u001b[39mqp(P, q, G, h, A, b)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# To flatten as one dimension array\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(solution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\cvxopt\\coneprog.py:4485\u001b[0m, in \u001b[0;36mqp\u001b[1;34m(P, q, G, h, A, b, solver, kktsolver, initvals, **kwargs)\u001b[0m\n\u001b[0;32m   4475\u001b[0m         pinfres, dinfres \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: status, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m: s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m: z,\n\u001b[0;32m   4478\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimal objective\u001b[39m\u001b[38;5;124m'\u001b[39m: pcost, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdual objective\u001b[39m\u001b[38;5;124m'\u001b[39m: dcost,\n\u001b[0;32m   4479\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgap\u001b[39m\u001b[38;5;124m'\u001b[39m: gap, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelative gap\u001b[39m\u001b[38;5;124m'\u001b[39m: relgap,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4482\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidual as primal infeasibility certificate\u001b[39m\u001b[38;5;124m'\u001b[39m: pinfres,\n\u001b[0;32m   4483\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidual as dual infeasibility certificate\u001b[39m\u001b[38;5;124m'\u001b[39m: dinfres}\n\u001b[1;32m-> 4485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m coneqp(P, q, G, h, \u001b[38;5;28;01mNone\u001b[39;00m, A,  b, initvals, kktsolver \u001b[38;5;241m=\u001b[39m kktsolver, options \u001b[38;5;241m=\u001b[39m options)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\cvxopt\\coneprog.py:2243\u001b[0m, in \u001b[0;36mconeqp\u001b[1;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: x,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: y,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m: s,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m: z,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: status,\n\u001b[0;32m   2230\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgap\u001b[39m\u001b[38;5;124m'\u001b[39m: gap,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelative gap\u001b[39m\u001b[38;5;124m'\u001b[39m: relgap,\n\u001b[0;32m   2231\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimal objective\u001b[39m\u001b[38;5;124m'\u001b[39m: pcost,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdual objective\u001b[39m\u001b[38;5;124m'\u001b[39m: dcost,\n\u001b[0;32m   2232\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimal infeasibility\u001b[39m\u001b[38;5;124m'\u001b[39m: pres,\n\u001b[0;32m   2233\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdual infeasibility\u001b[39m\u001b[38;5;124m'\u001b[39m: dres, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimal slack\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39mts,\n\u001b[0;32m   2234\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdual slack\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39mtz , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: iters }\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;66;03m# Compute initial scaling W and scaled iterates:\u001b[39;00m\n\u001b[0;32m   2238\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;66;03m#     W * z = W^{-T} * s = lambda.\u001b[39;00m\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   2241\u001b[0m \u001b[38;5;66;03m# lmbdasq = lambda o lambda.\u001b[39;00m\n\u001b[1;32m-> 2243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  W \u001b[38;5;241m=\u001b[39m misc\u001b[38;5;241m.\u001b[39mcompute_scaling(s, z, lmbda, dims)\n\u001b[0;32m   2244\u001b[0m misc\u001b[38;5;241m.\u001b[39mssqr(lmbdasq, lmbda, dims)\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;66;03m# f3(x, y, z) solves\u001b[39;00m\n\u001b[0;32m   2248\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;66;03m#    [ P   A'  G'    ] [ ux        ]   [ bx ]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;66;03m# On entry, x, y, z containg bx, by, bz.\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \u001b[38;5;66;03m# On exit, they contain ux, uy, uz.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\cvxopt\\misc.py:285\u001b[0m, in \u001b[0;36mcompute_scaling\u001b[1;34m(s, z, lmbda, dims, mnl)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# For the 'l' block: \u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m#     W['d'] = sqrt( sk ./ zk )\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# where sk and zk are the first dims['l'] entries of s and z.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# lambda_k is stored in the first dims['l'] positions of lmbda.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m m \u001b[38;5;241m=\u001b[39m dims[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 285\u001b[0m W[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39msqrt( base\u001b[38;5;241m.\u001b[39mdiv( s[mnl:mnl\u001b[38;5;241m+\u001b[39mm], z[mnl:mnl\u001b[38;5;241m+\u001b[39mm] ))\n\u001b[0;32m    286\u001b[0m W[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m W[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    287\u001b[0m lmbda[mnl:mnl\u001b[38;5;241m+\u001b[39mm] \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39msqrt( base\u001b[38;5;241m.\u001b[39mmul( s[mnl:mnl\u001b[38;5;241m+\u001b[39mm], z[mnl:mnl\u001b[38;5;241m+\u001b[39mm] ) ) \n",
      "\u001b[1;31mValueError\u001b[0m: domain error"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import cvxopt.solvers\n",
    "import numpy.linalg as la\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "MIN_SUPPORT_VECTOR_MULTIPLIER = 1e-5\n",
    "\n",
    "class Kernel(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        # Implementing the linear method relation between two features x and y\n",
    "        return lambda x,y:np.dot(x.T,y)\n",
    "\n",
    "    @staticmethod\n",
    "    def polykernel(dimension, offset):\n",
    "        return lambda x, y: ((offset + np.dot(x.T,y)) ** dimension)\n",
    "\n",
    "    @staticmethod\n",
    "    def radial_basis(gamma):\n",
    "        return lambda x, y: np.exp(-gamma*la.norm(np.subtract(x, y)))\n",
    "\n",
    "class SVMTrainer(object):\n",
    "\n",
    "    def __init__(self, kernel, c):\n",
    "        # Assigning the attributes kernal and C value\n",
    "        self.kernel = kernel\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Training function\n",
    "        # Caluculate the langrange multipliers\n",
    "        lagrange_multipliers = self.compute_multipliers(X, y)\n",
    "        # Trainer returns SVM predictor that is used to predict which class the test element belongs to\n",
    "        return self.construct_predictor(X, y, lagrange_multipliers)\n",
    "\n",
    "    def kernel_matrix(self, X, n_samples):\n",
    "        # Size of kernal matrix is (no_of_inputs , no_of_inputs)\n",
    "        # Reason for this is that kernel function value is calculated between every 2 inputs given\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i, x_i in enumerate(X):\n",
    "            for j, x_j in enumerate(X):\n",
    "                K[i, j] = self.kernel(x_i, x_j)\n",
    "        #  Returns the kernel function values\n",
    "        return K\n",
    "\n",
    "    def construct_predictor(self, X, y, lagrange_multipliers):\n",
    "        support_vector_indices = lagrange_multipliers > MIN_SUPPORT_VECTOR_MULTIPLIER\n",
    "        support_multipliers = lagrange_multipliers[support_vector_indices]\n",
    "        support_vectors = X[support_vector_indices]\n",
    "        support_vector_labels = y[support_vector_indices]\n",
    "        bias = np.mean(\n",
    "            [y_k - SVMPredictor(\n",
    "                kernel=self.kernel,\n",
    "                bias=0.0,\n",
    "                weights=support_multipliers,\n",
    "                support_vectors=support_vectors,\n",
    "                support_vector_labels=support_vector_labels).predict(x_k)\n",
    "             for (y_k, x_k) in zip(support_vector_labels, support_vectors)])\n",
    "        return SVMPredictor(\n",
    "            kernel=self.kernel,\n",
    "            bias=bias,\n",
    "            weights=support_multipliers,\n",
    "            support_vectors=support_vectors,\n",
    "            support_vector_labels=support_vector_labels)\n",
    "\n",
    "    def compute_multipliers(self, X, y):\n",
    "        # n_samples is no_of_inputs\n",
    "        # n_features is no_of_features\n",
    "        n_samples, n_features = X.shape\n",
    "        # Returns kernel function matrix\n",
    "        K = self.kernel_matrix(X,n_samples)\n",
    "        # np.outer(a,b) gives\n",
    "        P = cvxopt.matrix(np.outer(y, y) * K)\n",
    "        q = cvxopt.matrix(-1 * np.ones(n_samples))\n",
    "        # Create a diagonal matrix of (n_samples , n_samples) dimension with -1 as value\n",
    "        G_std = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "        h_std = cvxopt.matrix(np.zeros(n_samples))\n",
    "        G_slack = cvxopt.matrix(np.diag(np.ones(n_samples)))\n",
    "        h_slack = cvxopt.matrix(np.ones(n_samples) * self.c)\n",
    "        G = cvxopt.matrix(np.vstack((G_std, G_slack)))\n",
    "        h = cvxopt.matrix(np.vstack((h_std, h_slack)))\n",
    "        A = cvxopt.matrix(y, (1, n_samples) , 'd')\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        # To flatten as one dimension array\n",
    "        return np.ravel(solution['x'])\n",
    "\n",
    "class SVMPredictor(object):\n",
    "\n",
    "    # Initializing the SVM predictor\n",
    "    # Needs kernel (K) , bias (b) , weights (w) , support_vectors and support_vector_labels\n",
    "    def __init__(self,\n",
    "                 kernel,\n",
    "                 bias,\n",
    "                 weights,\n",
    "                 support_vectors,\n",
    "                 support_vector_labels):\n",
    "        self._kernel = kernel\n",
    "        self._bias = bias\n",
    "        self._weights = weights\n",
    "        self._support_vectors = support_vectors\n",
    "        self._support_vector_labels = support_vector_labels\n",
    "        assert len(support_vectors) == len(support_vector_labels)\n",
    "        # Weights is equal to support vector labels as per mathematical formulation\n",
    "        assert len(weights) == len(support_vector_labels)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # result = b\n",
    "        result = self._bias\n",
    "        for z_i, x_i, y_i in zip(self._weights,\n",
    "                                 self._support_vectors,\n",
    "                                 self._support_vector_labels):\n",
    "            # result += w * support_vector_labels * K\n",
    "            result += z_i * y_i * self._kernel(x_i, x)\n",
    "        # Returning the sign of the value predicted\n",
    "        # +1 means belonging to positive (non spam) class\n",
    "        # -1 means belonging to negative (spam) class\n",
    "        return np.sign(result).item()\n",
    "\n",
    "def calculate(true_positive,false_positive,false_negative,true_negative):\n",
    "    result = {}\n",
    "    result['precision'] = true_positive / (true_positive + false_positive)\n",
    "    result['recall'] = true_positive / (true_positive + false_negative)\n",
    "    return result\n",
    "\n",
    "def confusion_matrix(true_positive,false_positive,false_negative,true_negative):\n",
    "    matrix = PrettyTable([' ', 'Ham' , 'Spam'])\n",
    "    matrix.add_row(['Ham', true_positive , false_positive])\n",
    "    matrix.add_row(['Spam', false_negative , true_negative])\n",
    "    return matrix , calculate(true_positive,false_positive,false_negative,true_negative)\n",
    "\n",
    "def implementSVM(X_train,Y_train,X_test,Y_test,parameters,type):\n",
    "    ham_spam = 0\n",
    "    spam_spam = 0\n",
    "    ham_ham = 0\n",
    "    spam_ham = 0\n",
    "    if(type==\"polykernel\"):\n",
    "        dimension = parameters['dimension']\n",
    "        offset = parameters['offset']\n",
    "        trainer = SVMTrainer(Kernel.polykernel(dimension,offset),0.1)\n",
    "        predictor = trainer.train(X_train,Y_train)\n",
    "    elif(type==\"linear\"):\n",
    "        trainer = SVMTrainer(Kernel.linear(),0.1)\n",
    "        predictor = trainer.train(X_train,Y_train)\n",
    "    for i in range(X_test.shape[0]):\n",
    "        ans = predictor.predict(X_test[i])\n",
    "        if(ans==-1 and Y_test[i]==-1):\n",
    "            spam_spam+=1\n",
    "        elif(ans==1 and Y_test[i]==-1):\n",
    "            spam_ham+=1\n",
    "        elif(ans==1 and Y_test[i]==1):\n",
    "            ham_ham+=1\n",
    "        elif(ans==-1 and Y_test[i]==1):\n",
    "            ham_spam+=1\n",
    "    return confusion_matrix(ham_ham,ham_spam,spam_ham,spam_spam)\n",
    "\n",
    "def write_to_file(matrix,result,parameters,type,start_time):\n",
    "    f = open(\"results.txt\",\"a\")\n",
    "    if(type==\"polykernel\"):\n",
    "        f.write(\"Polykernel model parameters\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Dimension : \" + str(parameters['dimension']))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Offset : \" + str(parameters['offset']))\n",
    "    elif(type==\"linear\"):\n",
    "        f.write(\"Linear model\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(matrix.get_string())\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Precision : \" + str(round(result['precision'],2)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Recall : \" + str(round(result['recall'],2)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Time spent for model : \" + str(round(time()-start_time,2)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "global_start_time = time()\n",
    "\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "df1 = pd.read_csv('wordslist.csv')\n",
    "df2 = pd.read_csv('frequency.csv',header=0)\n",
    "\n",
    "input_output = df2.to_numpy()\n",
    "X = input_output[:,:-1]\n",
    "Y = input_output[:,-1:]\n",
    "\n",
    "total = X.shape[0]\n",
    "train = int(X.shape[0] * 70 / 100)\n",
    "\n",
    "X_train = X[:train,:]\n",
    "Y_train = Y[:train,:]\n",
    "X_test = X[train:,:]\n",
    "Y_test = Y[train:,:]\n",
    "\n",
    "f = open(\"results.txt\",\"w+\")\n",
    "f.close()\n",
    "k=0\n",
    "\n",
    "type = {}\n",
    "parameters = {}\n",
    "\n",
    "type['1'] = \"polykernel\"\n",
    "type['2'] = \"linear\"\n",
    "\n",
    "for i in range(2,4):\n",
    "    for j in range(0,10):\n",
    "        start_time = time()\n",
    "        parameters['dimension'] = i\n",
    "        parameters['offset'] = j\n",
    "        matrix , result = implementSVM(X_train,Y_train,X_test,Y_test,parameters,str(type['1']))\n",
    "        write_to_file(matrix,result,parameters,type,start_time)\n",
    "        k+=1\n",
    "        print(\"Done : \" + str(k))\n",
    "\n",
    "start_time = time()\n",
    "matrix , result = implementSVM(X_train,Y_train,X_test,Y_test,parameters,str(type['2']))\n",
    "write_to_file(matrix,result,parameters,type,start_time)\n",
    "k+=1\n",
    "print(\"Done : \" + str(k))\n",
    "\n",
    "f = open(\"results.txt\",\"a\")\n",
    "f.write(\"Time spent for entire code : \" + str(round(time()-global_start_time,2)))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f729ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac286f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
